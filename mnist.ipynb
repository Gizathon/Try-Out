{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import logging\n",
    "from scipy.ndimage import zoom\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 196  # 14x14\n",
    "hidden_size = 10 \n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images):\n",
    "    return np.array([zoom(image[0], (0.5, 0.5)) for image in images])\n",
    "\n",
    "def prepare_datasets():\n",
    "    print(\"Prepare dataset...\")\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False)\n",
    "\n",
    "    x_train = resize_images(train_dataset)\n",
    "    x_test = resize_images(test_dataset)\n",
    "\n",
    "    x_train = torch.tensor(x_train.reshape(-1, 14*14).astype('float32') / 255)\n",
    "    y_train = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "\n",
    "    x_test = torch.tensor(x_test.reshape(-1, 14*14).astype('float32') / 255)\n",
    "    y_test = torch.tensor([label for _, label in test_dataset], dtype=torch.long)\n",
    "\n",
    "    print(\"âœ… Datasets prepared successfully\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(x_train, y_train, x_test, y_test):\n",
    "    print(\"Create loaders...\")\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"âœ… Loaders created!\")\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader):\n",
    "    print(\"Train model...\")\n",
    "\n",
    "    model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device).reshape(-1, 14*14)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(\"âœ… Model trained successfully\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    print(\"Test model...\")\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device).reshape(-1, 14*14)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataset...\n",
      "âœ… Datasets prepared successfully\n",
      "Create loaders...\n",
      "âœ… Loaders created!\n",
      "Train model...\n",
      "Epoch [1/10], Step [100/235], Loss: 1.8380\n",
      "Epoch [1/10], Step [200/235], Loss: 1.2667\n",
      "Epoch [2/10], Step [100/235], Loss: 0.7495\n",
      "Epoch [2/10], Step [200/235], Loss: 0.6124\n",
      "Epoch [3/10], Step [100/235], Loss: 0.4208\n",
      "Epoch [3/10], Step [200/235], Loss: 0.5229\n",
      "Epoch [4/10], Step [100/235], Loss: 0.3901\n",
      "Epoch [4/10], Step [200/235], Loss: 0.3713\n",
      "Epoch [5/10], Step [100/235], Loss: 0.3010\n",
      "Epoch [5/10], Step [200/235], Loss: 0.3885\n",
      "Epoch [6/10], Step [100/235], Loss: 0.2924\n",
      "Epoch [6/10], Step [200/235], Loss: 0.3261\n",
      "Epoch [7/10], Step [100/235], Loss: 0.2697\n",
      "Epoch [7/10], Step [200/235], Loss: 0.3028\n",
      "Epoch [8/10], Step [100/235], Loss: 0.3758\n",
      "Epoch [8/10], Step [200/235], Loss: 0.2732\n",
      "Epoch [9/10], Step [100/235], Loss: 0.2874\n",
      "Epoch [9/10], Step [200/235], Loss: 0.3152\n",
      "Epoch [10/10], Step [100/235], Loss: 0.3307\n",
      "Epoch [10/10], Step [200/235], Loss: 0.2918\n",
      "âœ… Model trained successfully\n",
      "Test model...\n",
      "Accuracy of the network on the 10000 test images: 91.49 %\n"
     ]
    }
   ],
   "source": [
    "def execution():\n",
    "    # Prepare training and testing datasets\n",
    "    x_train, y_train, x_test, y_test = prepare_datasets()\n",
    "\n",
    "    train_loader, test_loader = create_data_loaders(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    model = train_model(train_loader)\n",
    " \n",
    "    test_model(model, test_loader)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been converted to ONNX and saved as mnist_model.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "def convert_to_onnx(model, onnx_file_path):\n",
    "    dummy_input = torch.randn(1, input_size).to(device)\n",
    "    torch.onnx.export(model, dummy_input, onnx_file_path,\n",
    "                      export_params=True, opset_version=10, do_constant_folding=True)\n",
    "\n",
    "    print(f\"Model has been converted to ONNX and saved as {onnx_file_path}\")\n",
    "\n",
    "onnx_file_path = \"mnist_model.onnx\"\n",
    "convert_to_onnx(model, onnx_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for the previous MNIST tutorial\n",
    "def preprocess_image(image_path: str):\n",
    "    \"\"\"\n",
    "    Preprocess an image for the MNIST model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load image, convert to grayscale, resize and normalize\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    # Resize to match the input size of the model\n",
    "    image = image.resize((14, 14))\n",
    "    image = np.array(image).astype('float32') / 255\n",
    "    image = image.reshape(1, 196)  # Reshape to (1, 196) for model input\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giza.agents.model import GizaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting deserialization process...\n",
      "âœ… Deserialization completed! ðŸŽ‰\n",
      "Result:  0\n",
      "Request id:  3055227ba523412c90fddc1597733bc5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, '3055227ba523412c90fddc1597733bc5')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ID = 651  # Update with your model ID\n",
    "VERSION_ID = 2  # Update with your version ID\n",
    "\n",
    "def prediction(image, model_id, version_id):\n",
    "    model = GizaModel(id=model_id, version=version_id)\n",
    "\n",
    "    (result, request_id) = model.predict(\n",
    "        input_feed={\"image\": image}, verifiable=True\n",
    "    )\n",
    "\n",
    "    # Convert result to a PyTorch tensor\n",
    "    result_tensor = torch.tensor(result)\n",
    "    # Apply softmax to convert to probabilities\n",
    "    temp = nn.Softmax(dim=1)\n",
    "    probabilities = temp(result_tensor)\n",
    "    # Use argmax to get the predicted class\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    return predicted_class.item(), request_id\n",
    "\n",
    "def execution():\n",
    "    image = preprocess_image(\"./imgs/zero.png\")\n",
    "    (result, request_id) = prediction(image, MODEL_ID, VERSION_ID)\n",
    "    print(\"Result: \", result)\n",
    "    print(\"Request id: \", request_id)\n",
    "\n",
    "    return result, request_id\n",
    "\n",
    "\n",
    "execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
